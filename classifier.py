import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
from torchvision.transforms import transforms
from PIL import Image
import numpy as np
from tqdm import tqdm
import re
import timm  # timm ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from sklearn.model_selection import KFold  # Cross-validationì„ ìœ„í•œ KFold ì¶”ê°€ (ì„ íƒì )
from sklearn.metrics import classification_report, confusion_matrix
import random

# --- 1. ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DATA_DIR = 'data'
IMAGE_SIZE = 224
BATCH_SIZE = 32
NUM_EPOCHS = 15
LEARNING_RATE = 1e-4
OUTPUT_DIR = 'classification_results'

CLASSES = {
    'Non Demented': 0,
    'Very mild Dementia': 1,
    'Mild Dementia': 2,
}
NUM_CLASSES = len(CLASSES)
IMAGE_CHANNEL = 1
MODEL_NAME = 'resnet18'  # ì‚¬ìš©í•  Pretrained ëª¨ë¸ ì´ë¦„

os.makedirs(OUTPUT_DIR, exist_ok=True)
BEST_MODEL_PATH = os.path.join(OUTPUT_DIR, f'best_classifier_{MODEL_NAME}_weights_42.pth')


# --- 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ---
class OasisCVAEDataset(Dataset):
    def __init__(self, data_dir, classes, transform=None):
        self.data_dir = data_dir
        self.classes = classes
        self.transform = transform
        self.image_paths = []
        self.labels = []
        self.subject_ids = []

        def extract_subject_id(filename):
            base_name = os.path.splitext(filename)[0]
            parts = base_name.split('_')
            if len(parts) >= 2:
                return '_'.join(parts[:2])
            return parts[0]

        for class_name, label_index in classes.items():
            class_dir = os.path.join(data_dir, class_name)
            if not os.path.exists(class_dir):
                continue
            for filename in os.listdir(class_dir):
                if filename.endswith(('.jpg', '.png', '.jpeg')):
                    img_path = os.path.join(class_dir, filename)
                    self.image_paths.append(img_path)
                    self.labels.append(label_index)
                    self.subject_ids.append(extract_subject_id(filename))

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('L')

        if self.transform:
            image = self.transform(image)

        label = self.labels[idx]
        return image, torch.tensor(label, dtype=torch.long)


# --- 3. ì´ë¯¸ì§€ ë³€í™˜ ì •ì˜ ---
transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.456], std=[0.224]),
])


# --- 4. Pretrained ë¶„ë¥˜ ëª¨ë¸ ì •ì˜ ---
class PretrainedClassifier(nn.Module):
    def __init__(self, model_name, num_classes, pretrained=True):
        super().__init__()
        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes, in_chans=1)

    def forward(self, x):
        return self.model(x)


def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

# --- 5. í•™ìŠµ ë° ê²€ì¦ í•¨ìˆ˜ ---
def train(model, data_loader, criterion, optimizer):
    model.train()
    running_loss = 0.0
    for inputs, labels in tqdm(data_loader, desc="Training"):
        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)
    return running_loss / len(data_loader.dataset)


def validate(model, data_loader, criterion):
    model.eval()
    running_loss = 0.0
    correct_predictions = 0
    all_labels = []
    all_predictions = []

    with torch.no_grad():
        for inputs, labels in tqdm(data_loader, desc="Validation"):
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs.data, 1)
            correct_predictions += (predicted == labels).sum().item()

            all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

    val_loss = running_loss / len(data_loader.dataset)
    val_accuracy = correct_predictions / len(data_loader.dataset)
    return val_loss, val_accuracy, all_labels, all_predictions


# --- 6. ë©”ì¸ ì‹¤í–‰ ---
if __name__ == '__main__':
    set_seed(42)
    full_dataset = OasisCVAEDataset(DATA_DIR, CLASSES, transform=transform)

    if len(full_dataset) > 0:
        subject_id_to_class = {}
        for i, sub_id in enumerate(full_dataset.subject_ids):
            subject_id_to_class[sub_id] = full_dataset.labels[i]

        subjects_by_class = {c: [] for c in CLASSES.values()}
        for sub_id, label in subject_id_to_class.items():
            subjects_by_class[label].append(sub_id)

        train_ratio = 0.8
        val_ratio = 0.2
        train_subjects = set()
        val_subjects = set()

        for label, subjects in subjects_by_class.items():
            np.random.shuffle(subjects)
            num_val = max(1, int(val_ratio * len(subjects)))
            val_split_subjects = subjects[:num_val]
            train_split_subjects = subjects[num_val:]

            train_subjects.update(train_split_subjects)
            val_subjects.update(val_split_subjects)

        train_indices = [i for i, sub_id in enumerate(full_dataset.subject_ids) if sub_id in train_subjects]
        val_indices = [i for i, sub_id in enumerate(full_dataset.subject_ids) if sub_id in val_subjects]

        # -----------------------------------------------------
        # â­ï¸ í•™ìŠµ(Train) í™˜ì IDë³„ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
        # -----------------------------------------------------
        print("\n--- â­ï¸ í•™ìŠµ(Train) í™˜ì IDë³„ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸ â­ï¸ ---")
        train_subject_class_map = {}
        reverse_classes = {v: k for k, v in CLASSES.items()}

        for i in train_indices:
            sub_id = full_dataset.subject_ids[i]
            true_label_index = full_dataset.labels[i]
            if sub_id not in train_subject_class_map:
                train_subject_class_map[sub_id] = reverse_classes[true_label_index]

        train_class_counts = {}
        for class_name in train_subject_class_map.values():
            train_class_counts[class_name] = train_class_counts.get(class_name, 0) + 1

        print(f"  ì´ í™˜ì ìˆ˜: {len(train_subject_class_map)}ëª…")
        print("\n  [ìš”ì•½] í•™ìŠµ ì„¸íŠ¸ í´ë˜ìŠ¤ë³„ í™˜ì ìˆ˜:")
        for class_name, count in train_class_counts.items():
            print(f"  - {class_name}: {count}ëª…")
        print("-" * 50)

        # -----------------------------------------------------
        # â­ï¸ ê²€ì¦(Validation) í™˜ì IDë³„ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
        # -----------------------------------------------------
        print("\n--- â­ï¸ ê²€ì¦(Validation) í™˜ì IDë³„ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸ â­ï¸ ---")
        val_subject_class_map = {}
        reverse_classes = {v: k for k, v in CLASSES.items()}

        for i in val_indices:
            sub_id = full_dataset.subject_ids[i]
            true_label_index = full_dataset.labels[i]
            if sub_id not in val_subject_class_map:
                val_subject_class_map[sub_id] = reverse_classes[true_label_index]
            elif val_subject_class_map[sub_id] != reverse_classes[true_label_index]:
                print(f"ê²½ê³ : í™˜ì ID {sub_id}ì— ëŒ€í•´ í´ë˜ìŠ¤ ë ˆì´ë¸”ì´ ë‹¤ë¦…ë‹ˆë‹¤. í™•ì¸ í•„ìš”!")

        for sub_id, class_name in sorted(val_subject_class_map.items()):
            print(f"  í™˜ì ID: {sub_id:<15} -> í´ë˜ìŠ¤: {class_name}")

        class_counts = {}
        for class_name in val_subject_class_map.values():
            class_counts[class_name] = class_counts.get(class_name, 0) + 1

        print("\n  [ìš”ì•½] ê²€ì¦ ì„¸íŠ¸ í´ë˜ìŠ¤ë³„ í™˜ì ìˆ˜:")
        for class_name, count in class_counts.items():
            print(f"  - {class_name}: {count}ëª…")
        print("-" * 50)

        # -----------------------------------------------------
        train_dataset = Subset(full_dataset, train_indices)
        val_dataset = Subset(full_dataset, val_indices)

        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True)

        all_subject_ids = list(subject_id_to_class.keys())
        total_subjects = len(all_subject_ids)

        print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ. ì´ í™˜ì ìˆ˜: {total_subjects}ëª…")
        print(f"í™˜ì ID ê¸°ë°˜ ë¶„í• : í•™ìŠµ ({len(train_subjects)}ëª…, {len(train_dataset)} ì´ë¯¸ì§€), "
              f"ê²€ì¦ ({len(val_subjects)}ëª…, {len(val_dataset)} ì´ë¯¸ì§€)")

        model = PretrainedClassifier(MODEL_NAME, NUM_CLASSES).to(DEVICE)
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)

        best_val_accuracy = 0.0
        print(f"\n--- {MODEL_NAME} ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (í™˜ì ID ë¶„í•  ì ìš©) ---")

        for epoch in range(1, NUM_EPOCHS + 1):
            train_loss = train(model, train_loader, criterion, optimizer)
            val_loss, val_accuracy, _, _ = validate(model, val_loader, criterion)

            print(f"Epoch {epoch}/{NUM_EPOCHS}")
            print(f"  Train Loss: {train_loss:.4f}")
            print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}")

            if val_accuracy > best_val_accuracy:
                best_val_accuracy = val_accuracy
                torch.save(model.state_dict(), BEST_MODEL_PATH)
                print(f"  âœ… Best model ì €ì¥ë¨. Val Acc: {best_val_accuracy:.4f} -> {BEST_MODEL_PATH}")

        print("\n--- í•™ìŠµ ì™„ë£Œ ---")
        print(f"ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_accuracy:.4f}, ê°€ì¤‘ì¹˜ ì €ì¥ ê²½ë¡œ: {BEST_MODEL_PATH}")

    else:
        print("ê²½ê³ : ì‹¤ì œ ë°ì´í„°ê°€ ì—†ì–´ ëª¨ë¸ ì •ì˜ë§Œ ì§„í–‰í•©ë‹ˆë‹¤. í•™ìŠµ ë° ê²€ì¦ì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")

        # --- 7. í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ë° ìµœì¢… í‰ê°€ (ì¶”ê°€ëœ ë¶€ë¶„) ---
    if len(full_dataset) > 0:
        print("\n" + "="*50)
        print(f"--- ğŸš€ ìµœì¢… í‰ê°€ ì‹œì‘: {MODEL_NAME} ---")
        print("="*50)

        # 1. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        final_model = PretrainedClassifier(MODEL_NAME, NUM_CLASSES).to(DEVICE)
        try:
            final_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE))
            print(f"âœ… {BEST_MODEL_PATH} ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ.")
        except FileNotFoundError:
            print(f"âŒ ì˜¤ë¥˜: í•™ìŠµëœ ê°€ì¤‘ì¹˜ íŒŒì¼ {BEST_MODEL_PATH}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("   ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•˜ì—¬ íŒŒì¼ì„ ì €ì¥í•˜ê±°ë‚˜, ì •í™•í•œ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            # íŒŒì¼ì´ ì—†ì„ ê²½ìš°, í‰ê°€ë¥¼ ì§„í–‰í•˜ì§€ ì•Šê³  ì¢…ë£Œ
            exit() 
        except Exception as e:
            print(f"âŒ ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            exit()

        # 2. ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ í‰ê°€ ìˆ˜í–‰
        print("\nâ¡ï¸ ê²€ì¦ ë°ì´í„°ì…‹(Validation Set)ì— ëŒ€í•œ ë¶„ë¥˜ ìˆ˜í–‰...")
        val_loss_final, val_accuracy_final, all_labels, all_predictions = validate(final_model, val_loader, criterion)
        
        # 3. ê²°ê³¼ ì¶œë ¥
        print("\n" + "-"*50)
        print(f"â­ ìµœì¢… ê²€ì¦ ì†ì‹¤ (Val Loss): {val_loss_final:.4f}")
        print(f"â­ ìµœì¢… ê²€ì¦ ì •í™•ë„ (Val Accuracy): {val_accuracy_final:.4f}")
        print("-" * 50)
        
        # 4. ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ ë° í˜¼ë™ í–‰ë ¬ ì¶œë ¥
        reverse_classes = {v: k for k, v in CLASSES.items()}
        # NUM_CLASSES (4)ë¥¼ ì‚¬ìš©í•˜ì—¬ 0, 1, 2, 3 ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ í´ë˜ìŠ¤ ì´ë¦„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        reverse_classes_list = [reverse_classes[i] for i in range(NUM_CLASSES)]

        print("\n--- ğŸ“Š ë¶„ë¥˜ ìƒì„¸ ë³´ê³ ì„œ (Classification Report) ---")
        print(classification_report(all_labels, all_predictions, target_names=reverse_classes_list, digits=4))

        print("\n--- ğŸ”¢ í˜¼ë™ í–‰ë ¬ (Confusion Matrix) ---")
        cm = confusion_matrix(all_labels, all_predictions)
        # np.arrayë¥¼ ë¬¸ìì—´ë¡œ ë³´ê¸° ì¢‹ê²Œ ë³€í™˜
        cm_str = np.array2string(cm, separator=', ', prefix='[', suffix=']')
        print("    [ì˜ˆì¸¡ëœ í´ë˜ìŠ¤]")
        print(f"    {cm_str}")
        print("    [ì‹¤ì œ í´ë˜ìŠ¤]")
        
        print("="*50)