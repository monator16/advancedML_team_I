{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08892a8c7ce8411cbda44ea02a131c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5322c732d11f4a65b2a35751e5688aaf",
              "IPY_MODEL_35ca342b5ab644b180f5dc141f7f8da4",
              "IPY_MODEL_e23463c20cf24cc884bd62e1520a4160"
            ],
            "layout": "IPY_MODEL_ba0122ecad8c42938e4b769f93bad3b4"
          }
        },
        "5322c732d11f4a65b2a35751e5688aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16ac47a69c0d44e1b8b58cb8fc69f5a0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dbb4fe0e05d1400e99d153485b48862a",
            "value": "ResNet50.pt:â€‡100%"
          }
        },
        "35ca342b5ab644b180f5dc141f7f8da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2076d13f77c14672b0e62661e05c2228",
            "max": 94345733,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92e50ff5e84245d6b1542a59bf6c29e1",
            "value": 94345733
          }
        },
        "e23463c20cf24cc884bd62e1520a4160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bc5cb8f97574e8686879ef21829f077",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_05d9696a52304fe3b82e0c1dbe50986b",
            "value": "â€‡94.3M/94.3Mâ€‡[00:02&lt;00:00,â€‡40.4MB/s]"
          }
        },
        "ba0122ecad8c42938e4b769f93bad3b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16ac47a69c0d44e1b8b58cb8fc69f5a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbb4fe0e05d1400e99d153485b48862a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2076d13f77c14672b0e62661e05c2228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92e50ff5e84245d6b1542a59bf6c29e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9bc5cb8f97574e8686879ef21829f077": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05d9696a52304fe3b82e0c1dbe50986b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "- ì°¾ì•„ë‚¸ best í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ ìž¬íŠœë‹"
      ],
      "metadata": {
        "id": "VOc7FjkROj1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm lpips torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s07BeLXMzv1",
        "outputId": "5f446804-3acb-455c-977f-04a2cf52ef41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.12/dist-packages (from lpips) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (1.16.3)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (4.67.1)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.11.12)\n",
            "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, lpips\n",
            "Successfully installed lightning-utilities-0.15.2 lpips-0.1.4 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--Colab setup---\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "# 1. ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 2. í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/25_AML_OASIS_dataset\"\n",
        "if not os.path.exists(PROJECT_DIR):\n",
        "    os.makedirs(PROJECT_DIR)\n",
        "os.chdir(PROJECT_DIR)\n",
        "print(f\"ìž‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")\n",
        "\n",
        "# 3. ë°ì´í„° ê²½ë¡œ ë¡œì»¬ë¡œ ë³µì‚¬\n",
        "DATA_DIR_SOURCE = os.path.join(PROJECT_DIR, \"input\")\n",
        "DATA_DIR = \"/content/input\"\n",
        "\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    shutil.copytree(DATA_DIR_SOURCE, DATA_DIR)\n",
        "\n",
        "print(f\" ì´ë¯¸ì§€ ë°ì´í„° ê²½ë¡œ: {DATA_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJkR-6uHM1x2",
        "outputId": "4cd052a4-a001-436e-fb8c-02d6fa3054b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "ìž‘ì—… ë””ë ‰í† ë¦¬: /content/drive/MyDrive/25_AML_OASIS_dataset\n",
            " ì´ë¯¸ì§€ ë°ì´í„° ê²½ë¡œ: /content/input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import lpips\n",
        "import timm\n",
        "import traceback\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
        "from torchmetrics.image import MultiScaleStructuralSimilarityIndexMeasure\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import random\n",
        "from scipy import linalg\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# --- ì„¤ì • ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMAGE_SIZE = 224\n",
        "IMAGE_CHANNEL = 1\n",
        "LATENT_DIM = 128\n",
        "CLASSES = {0: 'Non Demented', 1: 'Very mild Dementia', 2: 'Mild Dementia'}\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "CLASS_NAMES_MAP = {v: k for k, v in CLASSES.items()}\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# Dataset\n",
        "# ==============================================================\n",
        "\n",
        "class OASISFileDataset(Dataset):\n",
        "    def __init__(self, root_dir, subject_list_file, transform=None):\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        with open(subject_list_file, 'r') as f:\n",
        "            valid_subjects = set(line.strip() for line in f.readlines())\n",
        "\n",
        "        for class_folder in os.listdir(root_dir):\n",
        "            if class_folder not in CLASS_NAMES_MAP:\n",
        "              continue\n",
        "            class_label = CLASS_NAMES_MAP[class_folder]\n",
        "\n",
        "\n",
        "            for img_path in glob(os.path.join(root_dir, class_folder, \"*.jpg\")):\n",
        "                name = os.path.basename(img_path).split('.')[0]\n",
        "                parts = name.split('_')\n",
        "                subj = parts[1] if len(parts) > 1 else name\n",
        "\n",
        "                if subj in valid_subjects:\n",
        "                    self.data.append((subj, img_path, class_label))\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        subj, img_path, y = self.data[idx]\n",
        "        img = Image.open(img_path).convert(\"L\")\n",
        "        if self.transform:\n",
        "          img = self.transform(img)\n",
        "        return img, y, subj\n",
        "\n",
        "# ==============================================================\n",
        "# Model Architecture\n",
        "# ==============================================================\n",
        "\n",
        "\n",
        "class FiLMCond(nn.Module):\n",
        "    def __init__(self, embed_dim, num_features):\n",
        "        super().__init__()\n",
        "        self.scale = nn.Linear(embed_dim, num_features)\n",
        "        self.shift = nn.Linear(embed_dim, num_features)\n",
        "\n",
        "    def forward(self, h, class_emb):\n",
        "        gamma = self.scale(class_emb).unsqueeze(-1).unsqueeze(-1)\n",
        "        beta = self.shift(class_emb).unsqueeze(-1).unsqueeze(-1)\n",
        "        return h * (1 + gamma) + beta\n",
        "\n",
        "def up_block(cin, cout):\n",
        "    return nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "        nn.Conv2d(cin, cout, 3, 1, 1),\n",
        "        nn.BatchNorm2d(cout),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "class ImprovedCVAE(nn.Module):\n",
        "    def __init__(self, latent_dim, img_size, img_channel, num_classes):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.cls_dim = 32\n",
        "        self.content_dim = latent_dim - self.cls_dim\n",
        "\n",
        "        self.enc_conv1 = nn.Sequential(nn.Conv2d(img_channel, 32, 4, 2, 1), nn.BatchNorm2d(32), nn.ReLU())\n",
        "        self.enc_conv2 = nn.Sequential(nn.Conv2d(32, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU())\n",
        "        self.enc_conv3 = nn.Sequential(nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU())\n",
        "        self.enc_conv4 = nn.Sequential(nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU())\n",
        "\n",
        "        self.flat = 256 * 14 * 14\n",
        "        self.fc_mu = nn.Linear(self.flat, latent_dim)\n",
        "        self.fc_log = nn.Linear(self.flat, latent_dim)\n",
        "\n",
        "        self.class_embed = nn.Embedding(num_classes, self.cls_dim)\n",
        "\n",
        "        self.film1 = FiLMCond(self.cls_dim, 128)\n",
        "        self.film2 = FiLMCond(self.cls_dim, 64)\n",
        "        self.film3 = FiLMCond(self.cls_dim, 32)\n",
        "\n",
        "        self.dec_fc = nn.Linear(latent_dim, self.flat)\n",
        "        self.dec1 = up_block(256 + 128, 128)\n",
        "        self.dec2 = up_block(128 + 64, 64)\n",
        "        self.dec3 = up_block(64, 32)\n",
        "        self.out = nn.Sequential(nn.Conv2d(32, 1, 3, 1, 1), nn.Sigmoid())\n",
        "\n",
        "    def reparam(self, mu, logv):\n",
        "        std = (0.5 * logv).exp()\n",
        "        return mu + torch.randn_like(std) * std\n",
        "\n",
        "    def forward(self, x, class_label):\n",
        "        e1 = self.enc_conv1(x)\n",
        "        e2 = self.enc_conv2(e1)\n",
        "        e3 = self.enc_conv3(e2)\n",
        "        e4 = self.enc_conv4(e3)\n",
        "\n",
        "        h = e4.view(x.size(0), -1)\n",
        "        mu = self.fc_mu(h)\n",
        "        logv = self.fc_log(h)\n",
        "        z = self.reparam(mu, logv)\n",
        "\n",
        "        z_content = z[:, :self.content_dim]\n",
        "        z_class_raw = z[:, self.content_dim:]\n",
        "\n",
        "        class_emb = self.class_embed(class_label)\n",
        "        z_final = torch.cat([z_content, class_emb], dim=1)\n",
        "\n",
        "        d = self.dec_fc(z_final).view(x.size(0), 256, 14, 14)\n",
        "        d = F.interpolate(d, size=e3.shape[2:], mode=\"bilinear\")\n",
        "        d = self.dec1(torch.cat([d, e3], dim=1))\n",
        "        d = self.film1(d, class_emb)\n",
        "\n",
        "        d = F.interpolate(d, size=e2.shape[2:], mode=\"bilinear\")\n",
        "        d = self.dec2(torch.cat([d, e2], dim=1))\n",
        "        d = self.film2(d, class_emb)\n",
        "\n",
        "        d = F.interpolate(d, size=(112, 112), mode=\"bilinear\")\n",
        "        d = self.dec3(d)\n",
        "        d = self.film3(d, class_emb)\n",
        "\n",
        "        out = self.out(F.interpolate(d, size=(224, 224), mode=\"bilinear\"))\n",
        "        return out, mu, logv, z_content, z_class_raw\n",
        "\n",
        "# ==============================================================\n",
        "# Loss\n",
        "# ==============================================================\n",
        "\n",
        "\n",
        "# -- clustering loss --\n",
        "\n",
        "class ImprovedClusteringLoss(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes, margin=2.0):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.margin = margin\n",
        "        self.centers = nn.Parameter(torch.randn(num_classes, latent_dim))\n",
        "        nn.init.xavier_uniform_(self.centers)\n",
        "\n",
        "    def forward(self, features, labels):\n",
        "        centers_batch = self.centers[labels]\n",
        "        center_loss = F.mse_loss(features, centers_batch)\n",
        "\n",
        "        separation_loss = 0\n",
        "        num_pairs = 0\n",
        "        for i in range(self.num_classes):\n",
        "            for j in range(i + 1, self.num_classes):\n",
        "                dist = F.pairwise_distance(self.centers[i].unsqueeze(0), self.centers[j].unsqueeze(0))\n",
        "                separation_loss += F.relu(self.margin - dist)\n",
        "                num_pairs += 1\n",
        "\n",
        "        if num_pairs > 0:\n",
        "            separation_loss /= num_pairs\n",
        "\n",
        "        return center_loss, separation_loss\n",
        "\n",
        "\n",
        "\n",
        "# --- classification loss: pretrained classifier ---\n",
        "\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, model_name='resnet18', num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=False,\n",
        "            num_classes=num_classes,\n",
        "            in_chans=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def load_classifier(path=\"best_classifier_resnet18_weights_42.pth\"):\n",
        "\n",
        "    cls = SimpleClassifier(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "    ckpt = torch.load(path, map_location=DEVICE)\n",
        "    cls.load_state_dict(ckpt)\n",
        "    cls.eval()\n",
        "    for p in cls.parameters():\n",
        "        p.requires_grad = False\n",
        "    return cls\n",
        "\n",
        "\n",
        "\n",
        "# --- LPIPS loss ---\n",
        "lpips_fn = lpips.LPIPS(net='alex').to(DEVICE)\n",
        "for p in lpips_fn.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Validation\n",
        "# ---------------------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, beta, clustering_loss_fn, classifier, config):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    sum_loss = 0\n",
        "\n",
        "    for x, y, _ in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        recon, mu, logv, z_content, z_class_raw = model(x, y)\n",
        "\n",
        "        # loss\n",
        "        bce = F.binary_cross_entropy(recon, x, reduction='mean')\n",
        "        kld = -0.5 * torch.mean(1 + logv - mu.pow(2) - logv.exp())\n",
        "        lp = lpips_fn(recon.repeat(1, 3, 1, 1), x.repeat(1, 3, 1, 1)).mean()\n",
        "        center_loss, sep_loss = clustering_loss_fn(z_class_raw, y)\n",
        "\n",
        "        # classifier loss\n",
        "        recon_norm = (recon - 0.456) / 0.224\n",
        "        cls_logits = classifier(recon_norm)\n",
        "        cls_loss = F.cross_entropy(cls_logits, y)\n",
        "\n",
        "        # total loss\n",
        "        loss = (config['lambda_recon'] * bce +\n",
        "                beta * kld +\n",
        "                config['lambda_lpips'] * lp +\n",
        "                config['w_center'] * center_loss +\n",
        "                config['w_sep'] * sep_loss +        # W_SEPARATION -> config['w_sep']\n",
        "                config['lambda_cls'] * cls_loss)    # CLASSIFIER_LOSS_WEIGHT -> config['lambda_cls']\n",
        "\n",
        "        bs = x.size(0)\n",
        "        total += bs\n",
        "        sum_loss += loss.item() * bs\n",
        "\n",
        "    return sum_loss / total\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Training\n",
        "# ---------------------------------------------------------------\n",
        "def train_improved_cvae(model, train_loader, val_loader, classifier, config):\n",
        "\n",
        "    epochs = config['epochs']\n",
        "    lr = config['lr']\n",
        "    beta_max = config['beta']\n",
        "\n",
        "    # ì €ìž¥ ê²½ë¡œ\n",
        "    save_path = f\"exp_{config['exp_name']}.pth\"\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "\n",
        "    clustering_loss_fn = ImprovedClusteringLoss(\n",
        "        latent_dim=model.cls_dim,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        margin=config['margin']\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    best_val = 1e9\n",
        "    patience = 10\n",
        "    no_improve = 0\n",
        "\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "\n",
        "        # KL Annealing\n",
        "        beta = min(beta_max, beta_max * epoch / max(1, epochs // 2))\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False)\n",
        "        for x, y, _ in pbar:\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            recon, mu, logv, z_content, z_class_raw = model(x, y)\n",
        "\n",
        "            bce = F.binary_cross_entropy(recon, x, reduction='mean')\n",
        "            kld = -0.5 * torch.mean(1 + logv - mu.pow(2) - logv.exp())\n",
        "            lp = lpips_fn(recon.repeat(1, 3, 1, 1), x.repeat(1, 3, 1, 1)).mean()\n",
        "            center_loss, sep_loss = clustering_loss_fn(z_class_raw, y)\n",
        "\n",
        "            recon_norm = (recon - 0.456) / 0.224\n",
        "            cls_logits = classifier(recon_norm)\n",
        "            cls_loss = F.cross_entropy(cls_logits, y)\n",
        "\n",
        "\n",
        "            loss = (config['lambda_recon'] * bce +\n",
        "                    beta * kld +\n",
        "                    config['lambda_lpips'] * lp +\n",
        "                    config['w_center'] * center_loss +\n",
        "                    config['w_sep'] * sep_loss +\n",
        "                    config['lambda_cls'] * cls_loss)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                \"loss\": f\"{loss.item():.3f}\",\n",
        "                \"cls\": f\"{cls_loss.item():.3f}\"\n",
        "            })\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "        val_loss = validate(\n",
        "            model, val_loader, beta,\n",
        "            clustering_loss_fn, classifier, config\n",
        "        )\n",
        "\n",
        "        print(f\"Ep {epoch}/{epochs} | Val Loss {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            torch.save({\n",
        "                \"model\": model.state_dict(),\n",
        "                \"cluster\": clustering_loss_fn.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"hparams\": config\n",
        "            }, save_path)\n",
        "            print(f\"Saved best: {save_path}\\n\")\n",
        "            no_improve = 0\n",
        "\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "\n",
        "    return clustering_loss_fn"
      ],
      "metadata": {
        "id": "2Y5pB4vxLlEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5235d20f-b12c-4058-cbdc-d3e4cbf2ec96"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 233M/233M [00:01<00:00, 191MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # (1)ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "    train_dataset = OASISFileDataset(DATA_DIR, \"train_subjects.txt\", transform=transform)\n",
        "    val_dataset = OASISFileDataset(DATA_DIR, \"val_subjects.txt\", transform=transform)\n",
        "\n",
        "    # (2) ì‹¤í—˜ ì„¤ì •\n",
        "    base_config = {\n",
        "        'epochs': 20,           # ì‹¤í—˜ìš© ì—í­\n",
        "        'batch_size': 32,\n",
        "        'lr': 1e-4,\n",
        "        'lambda_recon': 1.0,\n",
        "        'beta': 2.0,\n",
        "        'lambda_lpips': 1.0,\n",
        "        'lambda_cls': 2.0,\n",
        "        'w_center': 10.0,\n",
        "        'w_sep': 5.0,\n",
        "        'margin': 2.0\n",
        "\n",
        "    }\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=base_config['batch_size'], shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=base_config['batch_size'], shuffle=False, num_workers=2)\n",
        "\n",
        "    # Classifier ë¡œë“œ\n",
        "    classifier = load_classifier()\n",
        "\n",
        "\n",
        "\n",
        "    experiments_list = [\n",
        "\n",
        "        # best1\n",
        "        {'exp_name': 'best1_lpips1.5_wsep0_mar1.8',  'lambda_lpips': 1.5, 'w_sep':0.0, 'margin':1.8},\n",
        "        # best 2\n",
        "        {'exp_name': 'best2_beta1_lpips2_wcen5_wsep0_cls1',  'beta': 1.0, 'lambda_lpips':2.0, 'w_center':5.0, 'w_sep':0.0, 'lambda_cls':1.0},\n",
        "        # best 3\n",
        "        {'exp_name': 'best3_wsep0_cls2.5_mar1.8', 'lambda_cls': 2.5, 'w_sep':0.0, 'margin':1.8},\n",
        "\n",
        "    ]\n",
        "\n",
        "    # (3) ë£¨í”„ ì‹¤í–‰\n",
        "\n",
        "\n",
        "    for i, exp_params in enumerate(experiments_list):\n",
        "        print(f\"[{i+1}/{len(experiments_list)}] for {exp_params['exp_name']}...\")\n",
        "\n",
        "        config = base_config.copy()\n",
        "        config.update(exp_params)\n",
        "\n",
        "        model = ImprovedCVAE(\n",
        "            LATENT_DIM, IMAGE_SIZE, IMAGE_CHANNEL, num_classes=NUM_CLASSES\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        try:\n",
        "            train_improved_cvae(model, train_loader, val_loader, classifier, config)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in experiment {config['exp_name']}: {e}\")\n",
        "\n",
        "    print(\"\\nëª¨ë“  ì‹¤í—˜ì´ ì™„ë£Œ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi1jtYDTUpLD",
        "outputId": "39e6e200-b2b5-4c29-a0cb-00db71bb5b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ ì´ 3ê°œì˜ ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œìž‘í•©ë‹ˆë‹¤.\n",
            "\n",
            "â–¶ [1/3] Setting up best1_lpips1.5_wsep0_mar1.8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1/20 | Val Loss 2.6937\n",
            "Saved best: exp_best1_lpips1.5_wsep0_mar1.8.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 2/20 | Val Loss 2.4170\n",
            "Saved best: exp_best1_lpips1.5_wsep0_mar1.8.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 3/20 | Val Loss 1.3185\n",
            "Saved best: exp_best1_lpips1.5_wsep0_mar1.8.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 4/20 | Val Loss 1.3125\n",
            "Saved best: exp_best1_lpips1.5_wsep0_mar1.8.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 5/20 | Val Loss 1.3693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 6/20 | Val Loss 1.4451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 7/20 | Val Loss 1.4831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 8/20 | Val Loss 1.5529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 9/20 | Val Loss 1.6207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 10/20 | Val Loss 1.6898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 11/20 | Val Loss 1.6577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 12/20 | Val Loss 1.6507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 13/20 | Val Loss 1.6320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 14/20 | Val Loss 1.6401\n",
            "Early stopping\n",
            "â–¶ [2/3] Setting up best2_beta1_lpips2_wcen5_wsep0_cls1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1/20 | Val Loss 1.7299\n",
            "Saved best: exp_best2_beta1_lpips2_wcen5_wsep0_cls1.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 2/20 | Val Loss 1.3086\n",
            "Saved best: exp_best2_beta1_lpips2_wcen5_wsep0_cls1.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 3/20 | Val Loss 1.2812\n",
            "Saved best: exp_best2_beta1_lpips2_wcen5_wsep0_cls1.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 4/20 | Val Loss 1.3147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 5/20 | Val Loss 0.9353\n",
            "Saved best: exp_best2_beta1_lpips2_wcen5_wsep0_cls1.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 6/20 | Val Loss 0.9541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 7/20 | Val Loss 0.9568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 8/20 | Val Loss 0.9800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 9/20 | Val Loss 1.0102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 10/20 | Val Loss 1.0487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 11/20 | Val Loss 1.0331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 12/20 | Val Loss 1.0253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 13/20 | Val Loss 1.0229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 14/20 | Val Loss 1.0191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 15/20 | Val Loss 1.0103\n",
            "Early stopping\n",
            "â–¶ [3/3] Setting up best3_wsep0_cls2.5_mar1.8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1/20 | Val Loss 2.9924\n",
            "Saved best: exp_best3_wsep0_cls2.5_mar1.8.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 2/20 | Val Loss 2.6453\n",
            "Saved best: exp_best3_wsep0_cls2.5_mar1.8.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 3/20 | Val Loss 1.6993\n",
            "Saved best: exp_best3_wsep0_cls2.5_mar1.8.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 4/20 | Val Loss 1.5114\n",
            "Saved best: exp_best3_wsep0_cls2.5_mar1.8.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 5/20 | Val Loss 1.5346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 6/20 | Val Loss 1.5530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 7/20 | Val Loss 1.5546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 8/20 | Val Loss 1.5476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 9/20 | Val Loss 1.5932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 10/20 | Val Loss 1.6481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 11/20 | Val Loss 1.6392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 12/20 | Val Loss 1.6239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 13/20 | Val Loss 1.6364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 14/20 | Val Loss 1.6211\n",
            "Early stopping\n",
            "\n",
            "ëª¨ë“  ì‹¤í—˜ì´ ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "bIbMtjeVNyTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval_1"
      ],
      "metadata": {
        "id": "IK-UTkmzEo6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# ì„¤ì •: ckpt íŒŒì¼ ë¡œì»¬ë¡œ ë³µì‚¬\n",
        "# ==============================================================\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "DRIVE_CKPT_DIR = \"/content/drive/MyDrive/25_AML_OASIS_dataset\" # ë“œë¼ì´ë¸Œ ê²½ë¡œ\n",
        "LOCAL_CKPT_DIR = \"/content/checkpoints\"                        # ë¡œì»¬ ìž„ì‹œ ê²½ë¡œ\n",
        "\n",
        "os.makedirs(LOCAL_CKPT_DIR, exist_ok=True)\n",
        "\n",
        "ckpt_files = glob(os.path.join(DRIVE_CKPT_DIR, \"exp_best*.pth\"))\n",
        "print(f\"{len(ckpt_files)}ê°œ ckpt íŒŒì¼\")\n",
        "\n",
        "# 3. ë³µì‚¬ ì‹¤í–‰\n",
        "for f in ckpt_files:\n",
        "    file_name = os.path.basename(f)\n",
        "    dest_path = os.path.join(LOCAL_CKPT_DIR, file_name)\n",
        "\n",
        "    if not os.path.exists(dest_path):\n",
        "        shutil.copy(f, LOCAL_CKPT_DIR)\n",
        "        print(f\"copied: {file_name}\")\n",
        "    else:\n",
        "        print(f\"skipped (exists): {file_name}\")\n"
      ],
      "metadata": {
        "id": "mdgJFszMTLuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e676d0be-6815-40b8-ee3c-60f23e93b247"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3ê°œ ckpt íŒŒì¼\n",
            "copied: exp_best1_lpips1.5_wsep0_mar1.8.pth\n",
            "copied: exp_best2_beta1_lpips2_wcen5_wsep0_cls1.pth\n",
            "copied: exp_best3_wsep0_cls2.5_mar1.8.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# ì‹œê°í™” ë° ì €ìž¥ í•¨ìˆ˜\n",
        "# ==============================================================\n",
        "@torch.no_grad()\n",
        "def save_comparison_plot(model, image, orig_label, target_label, save_path):\n",
        "    model.eval()\n",
        "    img = image.to(DEVICE).unsqueeze(0)\n",
        "    target = torch.tensor([target_label], device=DEVICE)\n",
        "    recon, _, _, _, _ = model(img, target)\n",
        "\n",
        "    x0_np = image.squeeze().cpu().numpy()\n",
        "    gen_np = recon.squeeze().cpu().clamp(0, 1).numpy()\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(x0_np, cmap='gray')\n",
        "    plt.title(f\"Orig ({CLASSES.get(orig_label.item())})\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(gen_np, cmap='gray')\n",
        "    plt.title(f\"Gen ({CLASSES.get(target_label.item())})\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "QoKHma3tNzMf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Evaluation 1: Image Quality Metrics\n",
        "# ==============================================================\n",
        "\n",
        "def calculate_psnr(img1, img2):\n",
        "    mse = torch.mean((img1 - img2) ** 2)\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "    return 20 * torch.log10(1.0 / torch.sqrt(mse)).item()\n",
        "\n",
        "def calculate_laplacian_sharpness(images):\n",
        "    kernel = torch.tensor([[0, 1, 0], [1, -4, 1], [0, 1, 0]],\n",
        "                         dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(images.device)\n",
        "    edge_map = F.conv2d(images, kernel, padding=1)\n",
        "    var = torch.var(edge_map.view(images.size(0), -1), dim=1)\n",
        "    return var.mean().item()\n",
        "\n",
        "def calculate_js_divergence(real_imgs, fake_imgs, bins=256):\n",
        "    real_pixels = (real_imgs.flatten() * 255).long()\n",
        "    fake_pixels = (fake_imgs.flatten() * 255).long()\n",
        "\n",
        "    hist_real = torch.histc(real_pixels.float(), bins=bins, min=0, max=255)\n",
        "    hist_fake = torch.histc(fake_pixels.float(), bins=bins, min=0, max=255)\n",
        "\n",
        "    p = hist_real / (hist_real.sum() + 1e-6)\n",
        "    q = hist_fake / (hist_fake.sum() + 1e-6)\n",
        "    m = 0.5 * (p + q)\n",
        "\n",
        "    kl_pm = F.kl_div((m + 1e-10).log(), p, reduction='batchmean')\n",
        "    kl_qm = F.kl_div((m + 1e-10).log(), q, reduction='batchmean')\n",
        "\n",
        "    js_div = 0.5 * kl_pm + 0.5 * kl_qm\n",
        "    return js_div.item()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iXt-hxOBwBW0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pretrained classifier for evaluation\n",
        "\n",
        "class PretrainedClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_classes, pretrained=True):\n",
        "        super().__init__()\n",
        "        # in_chans=1 ì„¤ì •ìœ¼ë¡œ 1ì±„ë„ ìž…ë ¥ ê°€ëŠ¥í•˜ê²Œ í•¨\n",
        "        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes, in_chans=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "def load_eval_classifier(path):\n",
        "\n",
        "    model = PretrainedClassifier(model_name='densenet121', num_classes=NUM_CLASSES, pretrained=False)\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    try:\n",
        "        ckpt = torch.load(path, map_location=DEVICE)\n",
        "        model.load_state_dict(ckpt)\n",
        "        print(\"classifier ok\")\n",
        "    except Exception as e:\n",
        "        print(f\"failed: {e}\")\n",
        "        return None\n",
        "\n",
        "    model.eval()\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "yf15wTtLfv4m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_all_checkpoints(\n",
        "    ckpt_dir,\n",
        "    val_list_path,\n",
        "    classifier_path,\n",
        "    data_root,\n",
        "    output_csv='final_evaluation_results.csv',\n",
        "    val_subset=0  # 0: ì „ì²´, >0: ì„œë¸Œì…‹\n",
        "):\n",
        "\n",
        "\n",
        "    lpips_metric = LearnedPerceptualImagePatchSimilarity(net_type='vgg').to(DEVICE)\n",
        "    ssim_metric = MultiScaleStructuralSimilarityIndexMeasure(data_range=1.0).to(DEVICE)\n",
        "    classifier = load_eval_classifier(classifier_path)\n",
        "\n",
        "    val_dataset = OASISFileDataset(data_root, val_list_path, transform=transform)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    ckpt_files = sorted(glob(os.path.join(ckpt_dir, \"exp_*.pth\")))\n",
        "    results = []\n",
        "\n",
        "\n",
        "    for ckpt_path in tqdm(ckpt_files, desc=\"Evaluating\"):\n",
        "      checkpoint = torch.load(ckpt_path, map_location=DEVICE)\n",
        "      exp_id = os.path.basename(ckpt_path).replace(\".pth\", \"\")\n",
        "\n",
        "      model = ImprovedCVAE(LATENT_DIM, IMAGE_SIZE, IMAGE_CHANNEL, NUM_CLASSES).to(DEVICE)\n",
        "      model.load_state_dict(checkpoint['model'])\n",
        "      model.eval()\n",
        "\n",
        "      cluster_loss_fn = ImprovedClusteringLoss(model.cls_dim, NUM_CLASSES).to(DEVICE)\n",
        "      if 'cluster' in checkpoint:\n",
        "          cluster_loss_fn.load_state_dict(checkpoint['cluster'])\n",
        "\n",
        "      hparams = checkpoint.get('hparams', {})\n",
        "\n",
        "      total_lpips = total_ssim = total_conf = total_acc = 0.0\n",
        "      total_lap = total_js = total_psnr = 0.0\n",
        "      n_samples = 0\n",
        "\n",
        "      z_collections = {c: [] for c in range(NUM_CLASSES)}\n",
        "\n",
        "      # for saving image & excels\n",
        "      exp_id = os.path.basename(ckpt_path).replace(\".pth\", \"\")\n",
        "      epoch = checkpoint.get('epoch', -1)\n",
        "      vis_save_dir = os.path.join(\"eval_outputs\", exp_id)\n",
        "      os.makedirs(vis_save_dir, exist_ok=True)\n",
        "\n",
        "      with torch.no_grad():\n",
        "            for imgs, labels, _ in val_loader:\n",
        "                imgs = imgs.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "                target_labels = (labels + 1) % NUM_CLASSES\n",
        "\n",
        "                recon, _, _, _, z_class_raw = model(imgs, target_labels)\n",
        "                recon_self, _, _, _, _ = model(imgs, labels) #for PSNR evaluation\n",
        "\n",
        "                recon_clipped = recon.clamp(0, 1)\n",
        "                recon_self_clipped = recon_self.clamp(0, 1)\n",
        "\n",
        "                # Metrics: regarding classification\n",
        "                if classifier:\n",
        "                    recon_norm = (recon - 0.456) / 0.224\n",
        "                    logits = classifier(recon_norm)\n",
        "                    probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "                    # confidence score\n",
        "                    target_probs = probs.gather(1, target_labels.view(-1, 1))\n",
        "                    total_conf += target_probs.sum().item()\n",
        "\n",
        "                    # accuracy\n",
        "                    preds = torch.argmax(probs, dim=1)\n",
        "                    total_acc += (preds == target_labels).sum().item()\n",
        "\n",
        "                # Metrics: LPIPS & SSIM\n",
        "                imgs_3c = imgs.repeat(1, 3, 1, 1)\n",
        "                recon_3c = recon_clipped.repeat(1, 3, 1, 1)\n",
        "\n",
        "                batch_lpips = lpips_metric(recon_3c, imgs_3c)\n",
        "                total_lpips += batch_lpips.item() * imgs.size(0)\n",
        "\n",
        "                batch_ssim = ssim_metric(recon_clipped, imgs)\n",
        "                total_ssim += batch_ssim.item() * imgs.size(0)\n",
        "\n",
        "                # Metrics: Laplacian, JS divergence, PSNR\n",
        "                total_lap += calculate_laplacian_sharpness(recon_clipped) * imgs.size(0)\n",
        "                total_js += calculate_js_divergence(imgs, recon_clipped) * imgs.size(0)\n",
        "                total_psnr += calculate_psnr(recon_self_clipped, imgs) * imgs.size(0)\n",
        "\n",
        "\n",
        "                for z, y in zip(z_class_raw, target_labels):\n",
        "                    z_collections[y.item()].append(z.cpu().numpy())\n",
        "\n",
        "                n_samples += imgs.size(0)\n",
        "\n",
        "                saved_count = 0\n",
        "                # Save images (Top 5 per model)\n",
        "                if saved_count < 5:\n",
        "                    for i in range(imgs.size(0)):\n",
        "                        if saved_count >= 5: break\n",
        "                        fname = f\"sample_{saved_count}.png\"\n",
        "                        save_comparison_plot(model, imgs[i], labels[i], target_labels[i],\n",
        "                                              os.path.join(vis_save_dir, fname))\n",
        "                        saved_count += 1\n",
        "\n",
        "            avg_lpips = total_lpips / n_samples\n",
        "            avg_ssim = total_ssim / n_samples\n",
        "            avg_conf = total_conf / n_samples if classifier else 0.0\n",
        "            avg_acc = total_acc / n_samples if classifier else 0.0\n",
        "            avg_psnr = total_psnr/ n_samples\n",
        "            avg_lap = total_lap / n_samples\n",
        "            avg_js = total_js / n_samples\n",
        "\n",
        "\n",
        "\n",
        "            # Compute Latent Metrics\n",
        "            intra_vars = []\n",
        "            for c in z_collections:\n",
        "                if len(z_collections[c]) > 1:\n",
        "                    arr = np.stack(z_collections[c])\n",
        "                    # Feature ì°¨ì›(32)ì— ëŒ€í•œ ë¶„ì‚°ì˜ í‰ê· \n",
        "                    intra_vars.append(np.mean(np.var(arr, axis=0)))\n",
        "            mean_intra_var = np.mean(intra_vars) if intra_vars else 0.0\n",
        "\n",
        "            centers = cluster_loss_fn.centers.detach()\n",
        "            centers = F.normalize(centers, dim=1)\n",
        "            dist_matrix = torch.cdist(centers, centers, p=2)\n",
        "            mask = ~torch.eye(NUM_CLASSES, dtype=bool, device=DEVICE)\n",
        "            mean_dist = dist_matrix[mask].mean().item()\n",
        "\n",
        "            # Result Dict\n",
        "            row = {\n",
        "                \"exp_id\": exp_id,\n",
        "                \"Target_Acc\": avg_acc,\n",
        "                \"Target_Conf\": avg_conf,\n",
        "                \"LPIPS\": avg_lpips,\n",
        "                \"MS_SSIM\": avg_ssim,\n",
        "                \"Intra_Var\": mean_intra_var,\n",
        "                \"Inter_Dist\": mean_dist,\n",
        "                \"Laplacian_Sharpness\": avg_lap,\n",
        "                \"JS_Divergence\": avg_js,\n",
        "                \"PSNR_Score\":avg_psnr,\n",
        "                **hparams\n",
        "            }\n",
        "            results.append(row)\n",
        "\n",
        "\n",
        "    # Save CSV\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "    priority = [\n",
        "        'exp_id',\n",
        "        'Target_Acc', 'Target_Conf',\n",
        "        'LPIPS', 'MS_SSIM',\n",
        "        'Laplacian_Sharpness', 'JS_Divergence',\n",
        "        'Intra_Var', 'Inter_Dist', 'PSNR_Score'\n",
        "    ]\n",
        "    others = [c for c in df.columns if c not in priority]\n",
        "    df = df[priority + others]\n",
        "\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    return df"
      ],
      "metadata": {
        "id": "rEPrHYZeSAyY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"[Evaluation Only] Starting full validation...\")\n",
        "\n",
        "    evaluate_all_checkpoints(\n",
        "        ckpt_dir= LOCAL_CKPT_DIR,\n",
        "        val_list_path=\"val_subjects.txt\",\n",
        "        classifier_path=\"best_classifier_densenet121_weights_42.pth\",\n",
        "        data_root=DATA_DIR,\n",
        "        output_csv=\"best_evaluation_results_1201.csv\",\n",
        "        val_subset=0\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89Iw-g7YODWs",
        "outputId": "5656a72b-5810-46e2-b443-4b893bd8860a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Evaluation Only] Starting full validation...\n",
            "classifier ok\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [04:09<00:00, 83.19s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval 2"
      ],
      "metadata": {
        "id": "mu_1zgVsFGEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================================\n",
        "# [Evaluation 2] Final Check: FMD, KID, C2ST(5-Fold)\n",
        "# =========================================================================\n",
        "\n",
        "# --- feature extractor ---\n",
        "class MedicalFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        try:\n",
        "            # RadImageNet ë¡œë“œ ì‹œë„\n",
        "            model_path = hf_hub_download(repo_id=\"Lab-Rasool/RadImageNet\", filename=\"ResNet50.pt\")\n",
        "            loaded_object = torch.load(model_path, map_location='cpu')\n",
        "            if isinstance(loaded_object, nn.Module): state_dict = loaded_object.state_dict()\n",
        "            else: state_dict = loaded_object\n",
        "\n",
        "            self.backbone = timm.create_model('resnet50', pretrained=False, num_classes=0)\n",
        "            self.backbone.load_state_dict(state_dict, strict=False)\n",
        "            print(\"ok\")\n",
        "        except:\n",
        "            print(\"Nope-> ImageNet\")\n",
        "            self.backbone = timm.create_model('resnet50', pretrained=True, num_classes=0)\n",
        "\n",
        "        self.backbone.eval()\n",
        "        for p in self.backbone.parameters(): p.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.shape[1] == 1: x = x.repeat(1, 3, 1, 1)\n",
        "        return self.backbone(x)\n",
        "\n",
        "# --- Metrics ---\n",
        "def calculate_fmd(feats1, feats2, eps=1e-6):\n",
        "    mu1, sigma1 = np.mean(feats1, axis=0), np.cov(feats1, rowvar=False)\n",
        "    mu2, sigma2 = np.mean(feats2, axis=0), np.cov(feats2, rowvar=False)\n",
        "    diff = mu1 - mu2\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "    if np.iscomplexobj(covmean): covmean = covmean.real\n",
        "    return float(diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean))\n",
        "\n",
        "def calculate_kid(feats1, feats2, degree=3, gamma=None, coef0=1.0):\n",
        "    def polynomial_kernel(X, Y):\n",
        "        if gamma is None: _gamma = 1.0 / X.shape[1]\n",
        "        else: _gamma = gamma\n",
        "        return (np.dot(X, Y.T) * _gamma + coef0) ** degree\n",
        "    m, n = feats1.shape[0], feats2.shape[0]\n",
        "    K_xx = polynomial_kernel(feats1, feats1)\n",
        "    K_yy = polynomial_kernel(feats2, feats2)\n",
        "    K_xy = polynomial_kernel(feats1, feats2)\n",
        "    mmd2 = (np.sum(K_xx)-np.trace(K_xx))/(m*(m-1)) + (np.sum(K_yy)-np.trace(K_yy))/(n*(n-1)) - 2*np.sum(K_xy)/(m*n)\n",
        "    return float(mmd2)\n",
        "\n",
        "def calculate_c2st_5fold(feats_real, feats_fake):\n",
        "    \"\"\"\n",
        "    Real(0) vs Fake(1) ë¶„ë¥˜ ì •í™•ë„ë¥¼ 5-Fold êµì°¨ ê²€ì¦ìœ¼ë¡œ ê³„ì‚°.\n",
        "    0.5ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ (êµ¬ë¶„ ë¶ˆê°€).\n",
        "    \"\"\"\n",
        "    X = np.concatenate([feats_real, feats_fake], axis=0)\n",
        "    y = np.concatenate([np.zeros(len(feats_real)), np.ones(len(feats_fake))], axis=0)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    # Logistic Regression (Fast)\n",
        "    clf = LogisticRegression(max_iter=1000, solver='lbfgs', n_jobs=-1)\n",
        "\n",
        "    for train_idx, test_idx in skf.split(X, y):\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        clf.fit(X_train, y_train)\n",
        "        acc = accuracy_score(y_test, clf.predict(X_test))\n",
        "        scores.append(acc)\n",
        "\n",
        "    return np.mean(scores), np.std(scores)\n",
        "\n",
        "\n",
        "# IC-LPIPS (Medical Feature Cosine Distance)\n",
        "def calculate_iclpips(feats_real, feats_fake):\n",
        "    \"\"\"\n",
        "    RadImageNet íŠ¹ì§• ê°„ì˜ ì½”ì‚¬ì¸ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•¨)\n",
        "    ë‡Œ êµ¬ì¡° ë³´ì¡´ì„ ì¸¡ì •\n",
        "    \"\"\"\n",
        "\n",
        "    norm_real = np.linalg.norm(feats_real, axis=1, keepdims=True)\n",
        "    norm_fake = np.linalg.norm(feats_fake, axis=1, keepdims=True)\n",
        "\n",
        "    norm_real = np.where(norm_real == 0, 1e-10, norm_real)\n",
        "    norm_fake = np.where(norm_fake == 0, 1e-10, norm_fake)\n",
        "\n",
        "    feats_real_norm = feats_real / norm_real\n",
        "    feats_fake_norm = feats_fake / norm_fake\n",
        "\n",
        "    similarity = np.sum(feats_real_norm * feats_fake_norm, axis=1)\n",
        "\n",
        "    distance = 1 - similarity\n",
        "\n",
        "    return float(np.mean(distance))\n",
        "\n",
        "def run_evaluation_2(ckpt_dir, data_root, val_list_path, output_csv=\"best_results_eval2_1201.csv\"):\n",
        "\n",
        "\n",
        "    extractor = MedicalFeatureExtractor().to(DEVICE)\n",
        "    ds_val = OASISFileDataset(data_root, val_list_path, transform=transform)\n",
        "    # ë°°ì¹˜ 128ë¡œ ë¹ ë¥´ê²Œ ì¶”ì¶œ\n",
        "    loader_val = DataLoader(ds_val, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "    real_feats = []\n",
        "    with torch.no_grad():\n",
        "        for x, _, _ in tqdm(loader_val, desc=\"Real Feats\"):\n",
        "            real_feats.append(extractor(x.to(DEVICE)).cpu().numpy())\n",
        "    real_feats = np.concatenate(real_feats, axis=0)\n",
        "\n",
        "\n",
        "    ckpt_files = sorted(glob(os.path.join(ckpt_dir, \"exp_best*.pth\")))\n",
        "    results = []\n",
        "\n",
        "    pbar = tqdm(ckpt_files, desc=\"Models\", unit=\"ckpt\")\n",
        "    for ckpt_path in pbar:\n",
        "        try:\n",
        "            exp_id = os.path.basename(ckpt_path).replace(\".pth\", \"\")\n",
        "            pbar.set_description(f\"Eval: {exp_id}\")\n",
        "\n",
        "\n",
        "            ckpt = torch.load(ckpt_path, map_location=DEVICE)\n",
        "            hparams = ckpt.get('hparams', {})\n",
        "            model = ImprovedCVAE(LATENT_DIM, IMAGE_SIZE, IMAGE_CHANNEL, NUM_CLASSES).to(DEVICE)\n",
        "            model.load_state_dict(ckpt['model'])\n",
        "            model.eval()\n",
        "\n",
        "\n",
        "            fake_feats = []\n",
        "            with torch.no_grad():\n",
        "                for x, y, _ in loader_val:\n",
        "                    x = x.to(DEVICE); y = y.to(DEVICE)\n",
        "                    target_y = (y + 1) % NUM_CLASSES\n",
        "                    recon, _, _, _, _ = model(x, target_y)\n",
        "                    fake_feats.append(extractor(recon).cpu().numpy())\n",
        "            fake_feats = np.concatenate(fake_feats, axis=0)\n",
        "\n",
        "            # Metrics\n",
        "            fmd = calculate_fmd(real_feats, fake_feats)\n",
        "            kid = calculate_kid(real_feats, fake_feats)\n",
        "            c2st_mean, c2st_std = calculate_c2st_5fold(real_feats, fake_feats)\n",
        "            iclpips = calculate_iclpips(real_feats, fake_feats)\n",
        "\n",
        "            row = {\n",
        "                \"exp_id\": exp_id,\n",
        "                \"FMD\": fmd,\n",
        "                \"KID\": kid,\n",
        "                \"C2ST_Mean\": c2st_mean,\n",
        "                \"C2ST_Std\": c2st_std,\n",
        "                \"ICLPIPS\": iclpips,\n",
        "                **hparams\n",
        "            }\n",
        "            results.append(row)\n",
        "            print(f\"   -> {exp_id} | FMD={fmd:.2f} | C2ST={c2st_mean:.3f}(Â±{c2st_std:.3f})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"error {e}\")\n",
        "\n",
        "    # Save\n",
        "    df = pd.DataFrame(results)\n",
        "    cols = ['exp_id', 'FMD', 'KID', 'ICLPIPS','C2ST_Mean', 'C2ST_Std'] + [c for c in df.columns if c not in ['exp_id', 'FMD', 'KID', 'C2ST_Mean', 'C2ST_Std']]\n",
        "    df = df[cols]\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(output_csv)\n",
        "    return df\n",
        "\n",
        "# ì‹¤í–‰\n",
        "if __name__ == \"__main__\":\n",
        "    run_evaluation_2(\n",
        "        ckpt_dir=\".\",\n",
        "        data_root=\"/content/input\",\n",
        "        val_list_path=\"val_subjects.txt\"\n",
        "    )"
      ],
      "metadata": {
        "id": "M13_3hVDeN6V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494,
          "referenced_widgets": [
            "08892a8c7ce8411cbda44ea02a131c71",
            "5322c732d11f4a65b2a35751e5688aaf",
            "35ca342b5ab644b180f5dc141f7f8da4",
            "e23463c20cf24cc884bd62e1520a4160",
            "ba0122ecad8c42938e4b769f93bad3b4",
            "16ac47a69c0d44e1b8b58cb8fc69f5a0",
            "dbb4fe0e05d1400e99d153485b48862a",
            "2076d13f77c14672b0e62661e05c2228",
            "92e50ff5e84245d6b1542a59bf6c29e1",
            "9bc5cb8f97574e8686879ef21829f077",
            "05d9696a52304fe3b82e0c1dbe50986b"
          ]
        },
        "outputId": "fe55a8ca-c528-4989-f3ad-abf475883a38"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ResNet50.pt:   0%|          | 0.00/94.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08892a8c7ce8411cbda44ea02a131c71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Real Feats: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:05<00:00,  2.13it/s]\n",
            "Eval: exp_best1_lpips1.5_wsep0_mar1.8:   0%|          | 0/3 [00:00<?, ?ckpt/s]/tmp/ipython-input-3507283142.py:35: DeprecationWarning: The `disp` argument is deprecated and will be removed in SciPy 1.18.0.\n",
            "  covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
            "/tmp/ipython-input-3507283142.py:35: LinAlgWarning: Matrix is singular. The result might be inaccurate or the array might not have a square root.\n",
            "  covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
            "Eval: exp_best2_beta1_lpips2_wcen5_wsep0_cls1:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:33<01:07, 33.75s/ckpt]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   -> exp_best1_lpips1.5_wsep0_mar1.8 | FMD=0.00 | C2ST=0.597(Â±0.013)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3507283142.py:35: DeprecationWarning: The `disp` argument is deprecated and will be removed in SciPy 1.18.0.\n",
            "  covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
            "/tmp/ipython-input-3507283142.py:35: LinAlgWarning: Matrix is singular. The result might be inaccurate or the array might not have a square root.\n",
            "  covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
            "Eval: exp_best3_wsep0_cls2.5_mar1.8:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:05<00:32, 32.43s/ckpt]          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   -> exp_best2_beta1_lpips2_wcen5_wsep0_cls1 | FMD=0.00 | C2ST=0.553(Â±0.023)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3507283142.py:35: DeprecationWarning: The `disp` argument is deprecated and will be removed in SciPy 1.18.0.\n",
            "  covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
            "/tmp/ipython-input-3507283142.py:35: LinAlgWarning: Matrix is singular. The result might be inaccurate or the array might not have a square root.\n",
            "  covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
            "Eval: exp_best3_wsep0_cls2.5_mar1.8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:34<00:00, 31.52s/ckpt]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   -> exp_best3_wsep0_cls2.5_mar1.8 | FMD=0.02 | C2ST=0.709(Â±0.008)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_results_eval2_1201.csv\n"
          ]
        }
      ]
    }
  ]
}